<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://sweetice.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sweetice.github.io/" rel="alternate" type="text/html" /><updated>2021-03-11T17:32:25-08:00</updated><id>https://sweetice.github.io/feed.xml</id><title type="html">Qiang He’s Homepage (CASIA)</title><subtitle>Homepage</subtitle><author><name>Qiang He (何强)</name></author><entry><title type="html">Functional Gradient Descent</title><link href="https://sweetice.github.io/posts/2021/03/functional-gradient-descent/" rel="alternate" type="text/html" title="Functional Gradient Descent" /><published>2021-03-12T00:00:00-08:00</published><updated>2021-03-12T00:00:00-08:00</updated><id>https://sweetice.github.io/posts/2021/03/functional_gradient_descent</id><content type="html" xml:base="https://sweetice.github.io/posts/2021/03/functional-gradient-descent/">&lt;p&gt;Claim: This is a backup of  &lt;a href=&quot;https://simple-complexities.github.io/optimization/functional/gradient/descent/2020/03/04/functional-gradient-descent.html&quot;&gt;this post&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;The content in this post has been adapted from &lt;a href=&quot;http://www.cs.cmu.edu/~16831-f12/notes/F12/16831_lecture21_danielsm.pdf&quot;&gt;Functional Gradient Descent - Part 1&lt;/a&gt; and &lt;a href=&quot;http://www.cs.cmu.edu/~16831-f14/notes/F10/16831_lecture24_varunnr/16831_lectureNov11.vramakri.pdf&quot;&gt;Part 2&lt;/a&gt;.
Functional Gradient Descent was introduced in the NIPS publication &lt;a href=&quot;https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf&quot;&gt;Boosting Algorithms as Gradient Descent&lt;/a&gt; by Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean in the year 2000.&lt;/p&gt;

&lt;p&gt;We are all familiar with gradient descent for linear functions \(f(x) = w^Tx\).&lt;br /&gt;
Once we define a loss \(L\), gradient descent does the following update steps (\(\eta\) is a parameter called the learning rate.):
\[
    w \rightarrow w - \eta \nabla L(w)
\]
where we move around in the space of weights.
An example of a loss \(L\) is:
\[
    L(w) = \sum_{i=1}^n(y_i - w^Tx_i)^2 + \lambda\lVert w \rVert ^2
\]
where the first term (the ‘L2’ term) measures how close \(f(x)\) is to \(y\), while the second term (the ‘regularization’ term) accounts for the ‘complexity’ of the learned function \(f\).&lt;/p&gt;

&lt;p&gt;Suppose we wanted to extend \(L\) to beyond linear functions \(f\). We want to minimize something like:
\[
    L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2
\]
where \(\lVert f \rVert ^2\) again serves as a regularization term, and we have updates of the form:
\[
    f \rightarrow f - \eta \nabla L(f)
\]
where we move around in the space of functions, not weights!&lt;/p&gt;

&lt;p&gt;Turns out, this is completely possible! And goes by the name of ‘functional’ gradient descent, or gradient descent in function space.&lt;/p&gt;

&lt;p&gt;A question that one may have is: why do this in the first place? Every function can be parametrized, and we can do ‘ordinary’ gradient descent in the space of parameters, instead?&lt;/p&gt;

&lt;p&gt;The answer is: yes, you always can! In general, you can parametrize any function in a number of ways, each parametrization gives rise to different steps (and different functions at each step) in gradient descent.&lt;/p&gt;

&lt;p&gt;The advantage is that some loss functions that are non-convex when parametrized, can be convex in the function space: this means functional gradient descent can actually converge to global minima, when ‘ordinary’ gradient descent could possibly get stuck at local minima or saddle points.&lt;/p&gt;

&lt;p&gt;So, what does functional gradient descent mean?&lt;/p&gt;

&lt;h3 id=&quot;functionals&quot;&gt;Functionals&lt;/h3&gt;
&lt;p&gt;A functional is a function defined over functions, returning a real value.&lt;br /&gt;
Examples:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The evaluation functional
\[
  E_x(f) = f(x)
\]&lt;/li&gt;
  &lt;li&gt;The sum functional
\[
  S_{{x_1, \ldots, x_n}}(f) = \sum_{i = 1}^n f(x_i) dx
\]&lt;/li&gt;
  &lt;li&gt;The integration functional
\[
  I_{[a, b]}(f) = \int_a^b f(x) dx
\]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It follows that the composition of a function \(g: \mathbb{R} \to \mathbb{R}\) with a functional is also a functional.
The loss function \(L(f)\) defined above is a functional!&lt;/p&gt;

&lt;h3 id=&quot;reproducing-kernel-hilbert-space&quot;&gt;Reproducing Kernel Hilbert Space&lt;/h3&gt;
&lt;p&gt;It turns out that it is especially convenient when functions come from a special set, called a reproducing kernel Hilbert space.
A Hilbert space can be thought of as a vector space, where we have the notion of an inner product between two elements. (This is not the complete definition, but this is what we’ll need).&lt;/p&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;review-kernels&quot;&gt;Review: Kernels&lt;/h4&gt;
&lt;p&gt;A kernel \(K: X \times X \to \mathbb{R}\) is a function that generalizes dot products:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Symmetry&lt;/em&gt;: For any \(x_i, x_j \in X\):
\[
  K(x_i, x_j) = K(x_j, x_i).
\]&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Positive Semi-Definiteness&lt;/em&gt;: For any \(x_1, \ldots, x_n \in X\), the matrix \(K_M\) given by 
\[
  K_{M_{ij}} = K(x_i, x_j)
\]
is positive semi-definite. Note that this implies \(K(x_i, x_j) \geq 0\) always.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It turns out (Mercer’s condition) that these conditions are equivalent to
\[
    K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
\]
where \(\phi\) is a function that is sometimes called the ‘feature map’. Thus, a kernel can be thought of as the dot product in some feature space (the range of \(\phi\)).
Similar to the dot product then, the kernel measures similarity between two inputs.&lt;/p&gt;

&lt;p&gt;Examples of kernel functions include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Linear Kernel: 
\[
  K(x_i, x_j) = x_i \cdot x_j 
\]&lt;/li&gt;
  &lt;li&gt;Polynomial Kernel (of degree \(d\)): 
\[
  K(x_i, x_j) = (x_i \cdot x_j + c)^d 
\]
The presence of the \(c\) term allows coefficients of degree less than \(d\) to be accommodated too.&lt;/li&gt;
  &lt;li&gt;RBF Kernel (of ‘width’ \(\sigma\)): 
\[
  K(x_i, x_j) = \exp\left(-\frac{\lVert x_i - x_j \rVert^2}{2\sigma^2}\right)
\]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Try to derive what the associated feature map is, for each of these kernels!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;We can now define a reproducing kernel Hilbert space or a ‘RKHS’.&lt;br /&gt;
A &lt;em&gt;reproducing kernel Hilbert space&lt;/em&gt;, obtained on fixing a kernel \(K\), is a space of functions where every function \(f\) is some linear combination of the kernel \(K\) evaluated at some ‘centers’ \(x_{Cf}\):
\[
    f(x) = \sum_{i = 1}^n \alpha_{f_i} K(x, x_{Cf_i}) 
\]
or, ignoring the argument \(x\):
\[
    f = \sum_{i = 1}^n \alpha_{f_i} K(\cdot, x_{Cf_i}) 
\]
For a kernel \(K\) will denote the associated reproducing kernel Hilbert space by \(H_K\).&lt;br /&gt;
From the definition above, every \(f \in H_K\) is completely determined by the coefficients \(\alpha_f\) and the centers \(x_{Cf}\). Note that the number of centers (\(=\) dimension of \(\alpha_f\)) can vary between functions.&lt;/p&gt;

&lt;p&gt;We can now define the inner product (the ‘dot’ product) in \(H_K\) by:
\[
    f \cdot g =  \sum_{i = 1}^{n_f} \sum_{j = 1}^{n_g} \alpha_{f_i} \alpha_{g_j} K(x_{Cf_i}, x_{Cg_j}) = \alpha_f K_{fg} \alpha_g
\]
where,
\[
    K_{{fg}_{ij}} = K(x_{Cf_i}, x_{Cg_j}).
\]
This inner product induces the norm \(\lVert \cdot \rVert\):
\[
    {\lVert f \rVert}^2 = f \cdot f = \alpha_f K_{ff} \alpha_f \geq 0.
\]
Why do we use the term reproducing? This is because we can ‘reproduce’ the value of \(f \in H_K\) at any \(x\) by taking the inner product of \(f\) with the ‘reproducing kernel’ function \(K(x, \cdot) \in H_K\):
\[
    f \cdot K(x, \cdot) = f(x).
\]
Verify this property!&lt;/p&gt;

&lt;p&gt;So, we’ve seen how to define the inner-product and norm \(\lVert f \rVert\) of any function \(f \in H_K\). But, in order to minimize via gradient descent, we need the definition of a derivative.&lt;/p&gt;

&lt;h3 id=&quot;derivatives-of-functionals&quot;&gt;Derivatives of Functionals&lt;/h3&gt;
&lt;p&gt;As reviewed in my previous post on &lt;a href=&quot;/optimization/constrained/theory/2020/03/03/optimization-review.html&quot;&gt;optimization theory&lt;/a&gt;, one of the definitions of the derivative \(Df\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{f(x + h) - (f(x) + Df(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0
\]
where \(Df(x)\) is a size \(n\) row vector, with which the take the dot product of the direction \(h\) with.&lt;/p&gt;

&lt;p&gt;We may not be working in \(\mathbb{R}^n\) anymore, but this definition gives us that the derivative \(DE\) of a functional \(E\) on \(H_K\) must satisfy:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(x + h) - (E(x) + DE(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0
\]
where \(h\) and \(x\) are now functions in \(H_K\), instead of points. This means that \(DE(x)\) is a function, too! (Recall that addition of functions occurs point-wise.)&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;Example 1:&lt;/strong&gt;
Let us take the example of the evaluation functional \(E_x(f) = f(x)\) and compute its derivative:
\[
    \begin{aligned}
        E(f + h) &amp;amp;= (f + h)(x)  \newline
                 &amp;amp;= f(x) + h(x) \newline
                 &amp;amp;= E(f) + h(x) \newline
                 &amp;amp;= E(f) + K(x, \cdot) \cdot h \newline
    \end{aligned}
\]
Thus, the derivative \(DE_x(f)\) is independent of \(f\), and is given by:
\[
        DE_x(f) = K(x, \cdot).
\]&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example 2:&lt;/strong&gt;
Similarly, following the example of my reference material, the functional \(E(f) = {\lVert f \rVert}^2\) satisfies:
\[
    \begin{aligned}
        E(f + h) &amp;amp;= {\lVert f + h \rVert}^2  \newline
                 &amp;amp;= {(f + h) \cdot (f + h)} \newline
                 &amp;amp;= {f \cdot f} + 2 {f \cdot h} + {h \cdot h} \newline
                 &amp;amp;= E(f) + 2 {f \cdot h} + {h \cdot h} \newline
    \end{aligned}
\]
Thus, the derivative \(DE(f)\) is defined as:
\[
        DE(f) = 2f.
\]
Note how similar this is to the derivative \(2x\) of the function \(x \to \lvert x \rvert^2\) on real numbers!&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt; &lt;/p&gt;

&lt;h4 id=&quot;the-chain-rule&quot;&gt;The Chain Rule&lt;/h4&gt;
&lt;p&gt;Very fortunately, we also have the chain rule!
As discussed before, if \(E\) is a functional and \(g: \mathbb{R} \to \mathbb{R}\) is differentiable, then \(g(E)\) is also a functional, with derivative:
\[
    D(g(E))(f) = g’(E(f)) \ DE(f).
\]&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Example 3:&lt;/strong&gt;
Let us compute the derivative of the loss functional \(L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2\) with the chain rule:
The individual terms in the first sum term is a composition of
\[
    g_i(x) = (y_i - x)^2 \text{ and } E_{x_i}.
\]
Thus, each of these terms has derivative:
\[
    \begin{aligned}
    D(g_i({x_i}))(f) &amp;amp;= -2 (y_i - E_{x_i}(f)) \cdot DE_{x_i}(f) \newline
                     &amp;amp;= -2 (y_i - f(x_i)) \cdot K(x_i, \cdot).
    \end{aligned}
\]
The second term has derivative \(2\lambda f\), as derived above.
Thus, the derivative \(DL(f)\) is given by:
\[
    DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f.
\]&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;There is one last point to note. When we take steps in ‘ordinary’ gradient descent, we move along the negative of the gradient vector because that is the direction along which the dot product with the gradient is minimum. No matter how this vector points! In some sense, we are not restricted to move in any direction. This is because our underlying domain is \(\mathbb{R}^n\).&lt;/p&gt;

&lt;p&gt;In ‘functional’ gradient descent, however, we are restricted to \(H_K\). How can we guarantee that when moving along \(DL(f)\), we do not stray out of \(H_K\)? One way to ensure that is by proving that we always have \(DL(f) \in H_K\). (This was true for our examples above! We have actually implicitly assumed this in our definition, too.) Then, closure of \(H_K\) under addition (and scalar multiplication) ensures that at every iteration, our current function \(f\) is in \(H_K\).&lt;/p&gt;

&lt;p&gt;This is what we will prove, in the next section.&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf&quot;&gt;Boosting Algorithms as Gradient Descent&lt;/a&gt; paper above, does not use reproducing kernel Hilbert spaces, and actually applies to more general sets of functions. This is why they mention the fact that moving along the gradient is not always possible. Instead, they move along the direction with the least dot product with the gradient, among all directions that keeps them within their domain of functions. With reproducing kernel Hilbert spaces, this is not a problem: we are, fortunately, not restricted to move along the negative of the gradient.&lt;/p&gt;

&lt;h3 id=&quot;h_k-is-closed-under-the-derivative&quot;&gt;\(H_K\) is Closed under the Derivative&lt;/h3&gt;
&lt;p&gt;If \(E\) is a functional on \(H_K\), and \(f \in H_K\), then we always have:
\[
    DE(f) \in H_K.
\]&lt;/p&gt;

&lt;p&gt;Let us define the derivative \(DE^*(f)\) as a functional:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(f + h) - (E(f) + DE^*(f)(h))}\rVert}{\lVert{h}\rVert} = 0
\]
I differentiate between \(DE^*(f)\) (the functional) and \(DE(f)\) (the function).
We want to show that, in fact:
\[
    DE^*(f)(h) = {\langle h, DE(f) \rangle}_K
\]&lt;/p&gt;

&lt;p&gt;Note that \(DE^*(f)\) is a linear functional!
Why? Using the definition above (and properties of the norm and limits):&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(DE^*(f)(ch) = c \cdot DE^*(f)(h)\) where \(c \in \mathbb{R}\).&lt;/li&gt;
  &lt;li&gt;\(DE^*(f)(h + g) = DE^*(f)(h) + DE^*(f)(g)\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This should not be surprising! We use \(DE^*(f)\) to give us the ‘best’ linear approximation around \(f\) along each direction \(h\).&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Riesz_representation_theorem&quot;&gt;Riesz Representation Theorem&lt;/a&gt; then tells us that every linear functional \(L\) on a Hilbert space is actually of the form:
\[
    L = \langle \cdot, v \rangle
\]
for some \(v\) in the Hilbert space, where \(\langle \cdot, \cdot \rangle\) is the inner product in the Hilbert space.&lt;br /&gt;
For an RKHS, the inner product is given by the kernel \(K\), so,
\[
    DE^*(f) = {\langle \cdot, DE(f) \rangle}_K
\]
for some \(DE(f) \in H_K\).
This means, we can write:
\[
    DE^*(f)(h) = {\langle h, DE(f) \rangle}_K
\]
where \(DE(f) \in H_K\), which is what we had to show!&lt;/p&gt;

&lt;h3 id=&quot;an-example&quot;&gt;An Example&lt;/h3&gt;
&lt;p&gt;Consider the regression problem, where \(x_i,\) for \(i \in {1, \ldots, 20}\) are linearly spaced in \([-1, 1]\):
\[
    y_i = e^{-\left(\frac{x_i - 0.5}{0.5}\right)^2} + e^{-\left(\frac{x_i + 0.5}{0.5}\right)^2} + \frac{\mathcal{N}(0, 1)}{20}
\]&lt;/p&gt;

&lt;p&gt;Although this is a simple enough problem that would be easily solved by ‘ordinary’ gradient descent, we will demonstrate how functional gradient descent works here.&lt;/p&gt;

&lt;p&gt;First, we need a loss function. We will use the L2 loss with regularization, \(L(f)\), defined above. We have already seen in Example 3, that the gradient of \(L(f)\) is:
\[
    DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f.
\]
Let us define \(K\) as the RBF kernel with width \(0.5\). The presence of the Gaussian noise term above means the true hypothesis is not in \(H_K\), but we should get close!&lt;/p&gt;

&lt;p&gt;We initialize \(\alpha_{f_0}\) randomly, and set:
\[ 
    f_0 = \sum_{i = 1}^{20} \alpha_{f_0i} K(\cdot, x_i) 
\]
and then start updating:
\[
    f_{t + 1} = f_t - \eta \cdot DL(f_t)
\]&lt;/p&gt;

&lt;p&gt;This makes sense! But if we want to represent this in code, we would have to represent these functions in some way. One way is to maintain the coefficients \(\alpha_{f_t}\) and kernel centers \(x_{C{f_t}}\) at every iteration. We can simplify this by deciding to store only \(\alpha_{f_t}\) (allowing zeros) and implicitly use all \(x_i\) as the kernel centers. This is actually the same as doing gradient descent on \(\alpha_f\)! Other ways would be to add training samples one-by-one in an online manner, maintaining/recomputing the function values only at the training points. However, this causes a complication wherein the function is only defined at the training samples. To fix this, instead of updating by the gradient, we update using smooth functions that approximate the gradient: this is exactly gradient boosting!&lt;/p&gt;

&lt;p&gt;If we decide to represent our function implicitly by \(\alpha_f\) at each step, our updates are now:
\[
    \alpha_{f_{t + 1}} = 2 \eta (y - f_t(x)) +  (1 - 2\lambda\eta) \alpha_{f_t}
\]
where \(y - f_t(x)\) is a vector with \((y - f_t(x))_i = y_i - f_t(x_i)\).
Check this!&lt;/p&gt;

&lt;p&gt;If we implement all this, and plot the resulting learned hypothesis at each step of gradient descent:&lt;/p&gt;

&lt;p style=&quot;text-align:center&quot;&gt;&lt;img src=&quot;/assets/images/functional_gradient_descent.gif&quot; alt=&quot;Functional Gradient Descent Example&quot; title=&quot;Functional Gradient Descent Example&quot; /&gt;&lt;/p&gt;

&lt;p&gt;indicating that functional gradient descent converges pretty fast, for our example.
The code for this example is available &lt;a href=&quot;https://github.com/simple-complexities/simple-complexities.github.io/tree/master/code/functional_gradient_descent.py&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We have seen:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Why functional gradient descent can be useful,&lt;/li&gt;
  &lt;li&gt;What it means to do functional gradient descent, and,&lt;/li&gt;
  &lt;li&gt;How we can do functional gradient descent, with an example.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;and that’s all I have for today.&lt;/p&gt;

&lt;p&gt;I’m also starting to introduce a commenting facility via GitHub Issues, in order to not clutter up this space here. Comment &lt;a href=&quot;https://github.com/simple-complexities/simple-complexities.github.io/issues/3&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;</content><author><name>Qiang He (何强)</name></author><category term="Optimization" /><summary type="html">Claim: This is a backup of this post!</summary></entry><entry><title type="html">配置ubuntu系统常用技能</title><link href="https://sweetice.github.io/posts/2019/12/skills-on-ubuntu/" rel="alternate" type="text/html" title="配置ubuntu系统常用技能" /><published>2019-12-25T00:00:00-08:00</published><updated>2019-12-25T00:00:00-08:00</updated><id>https://sweetice.github.io/posts/2019/12/Config_static_ipv6</id><content type="html" xml:base="https://sweetice.github.io/posts/2019/12/skills-on-ubuntu/">&lt;h2 id=&quot;配置静态ipv6地址&quot;&gt;配置静态ipv6地址&lt;/h2&gt;

&lt;p&gt;首先进入编辑界面&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vi /etc/network/interfaces
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;再添加&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;iface eth0 inet6 static
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;配置新用户&quot;&gt;配置新用户&lt;/h2&gt;

&lt;p&gt;这里推荐这种方式&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;useradd -d /home/user_name -m user_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;在home/下创建一个user_name 目录&lt;/p&gt;

&lt;p&gt;之后再配置密码&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;passwd user_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;为该用户指定命令解释程序（通常为/bin/bash）&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;usermod -s /bin/bash user_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;删除账号&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;userdel -r user_name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;-r 参数为删除该目录下所有的文件&lt;/p&gt;

&lt;h2 id=&quot;ssh-相关服务&quot;&gt;SSH 相关服务&lt;/h2&gt;

&lt;h3 id=&quot;指定ssh端口映射&quot;&gt;指定ssh端口映射:&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh -L 6006:localhost:6006 username@ip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;杀死占用某个端口的程序&quot;&gt;杀死占用某个端口的程序：&lt;/h3&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fuser 6006/tcp -k
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;快捷ssh命令访问服务器&quot;&gt;快捷ssh命令访问服务器&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;生成ssh 公钥
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh-keygen
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;上传到服务器
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scp -r ~/.ssh/id_rsa.pub  mirror@170.18.40.99:~/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;写入
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cat id_rsa.pub &amp;gt;&amp;gt; ~/.ssh/authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;在本地处理
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;vim ~/.ssh/config
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Host alians
  HostName dev.example.com
  Port 22
  User fooey
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;修改文件权限
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo chmod 600 .ssh/config 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://nerderati.com/2011/03/17/simplify-your-life-with-an-ssh-config-file/&quot;&gt;参考&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;限定服务器python程序数量&quot;&gt;限定服务器python程序数量&lt;/h2&gt;

&lt;p&gt;原理是替换掉python程序启动方式&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;alias python='run-one python'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Qiang He (何强)</name></author><category term="skills" /><summary type="html">配置静态ipv6地址</summary></entry><entry><title type="html">How to plot performance figures in reinforcement learning papers</title><link href="https://sweetice.github.io/posts/2019/04/plot-performance-figures-in-rl/" rel="alternate" type="text/html" title="How to plot performance figures in reinforcement learning papers" /><published>2019-04-19T00:00:00-07:00</published><updated>2019-04-19T00:00:00-07:00</updated><id>https://sweetice.github.io/posts/2019/04/How-to-plot-figures-like-RL-papers</id><content type="html" xml:base="https://sweetice.github.io/posts/2019/04/plot-performance-figures-in-rl/">&lt;h1 id=&quot;如何绘制出强化学习论文里面的曲线图&quot;&gt;如何绘制出强化学习论文里面的曲线图&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/10/ATk7lt.png&quot; alt=&quot;论文里面好看的图&quot; /&gt;&lt;/p&gt;

&lt;p&gt;paper里面含有精美的图能够增加文章中的概率, 那么强化学习论文里面那种精美的图到底是怎么画出来的呢?&lt;br /&gt;
答案是:使用seaborn绘制. 有人也提出来,可以使用origin绘制,但考虑到数据从python体系内转换到origin格式. 需要借助一些中介工具(比如excel), 这样子会增加难度.因此我们还是考虑使用seaborn来绘制.&lt;/p&gt;

&lt;p&gt;seaborn风格的图片如下:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/10/ATABB8.png&quot; alt=&quot;seaborn&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;绘图需要用到的数据&quot;&gt;绘图需要用到的数据&lt;/h2&gt;

&lt;p&gt;在RL里面,我们用来画图的数据一般包含:算法名字(用来分类), 平均reward回报, 随机种子(用来解决), 以及step次数.&lt;/p&gt;

&lt;p&gt;我们使用随机数据来生成这样的图片:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

time = np.linspace(0,200,201).reshape(-1,1)
Algo = ['PPO', 'DDPG', 'TRPO', 'DDPG-SG']


all = pd.DataFrame([])
for seed in range(10):
    for algo in Algo:
        data = pd.DataFrame(np.ones((201, 4)))
        data.columns = ['step', 'algo', 'avg_reward', 'seed']
        data['step'] = time
        data['algo'] = algo
        data['avg_reward'] = time + np.random.randn(201,1)*10
        data['seed'] = seed
        all = pd.concat([all, data], 0)

sns.lineplot(x='step', y='avg_reward', data=all, hue='algo')
plt.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;得到的效果:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s2.ax1x.com/2019/04/10/ATA6hj.png&quot; alt=&quot;效果&quot; /&gt;&lt;/p&gt;

&lt;p&gt;api详细信息请点&lt;a href=&quot;https://seaborn.pydata.org/generated/seaborn.lineplot.html#seaborn.lineplot&quot;&gt;此处&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;api细节&quot;&gt;API细节&lt;/h2&gt;

&lt;p&gt;lineplot带有大量的参数&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-angular2html&quot;&gt;seaborn.lineplot(x=None, y=None, hue=None, size=None, 
style=None, data=None, palette=None, hue_order=None, 
hue_norm=None, sizes=None, size_order=None, size_norm=None, 
dashes=True, markers=None, style_order=None, units=None, 
estimator='mean', ci=95, n_boot=1000, sort=True, 
err_style='band', err_kws=None, legend='brief', ax=None, **kwargs)¶
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;但是我们需要用到的参数就是那么几个&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;x&lt;br /&gt;
输入的横坐标,注意横坐标的取值不一定是唯一的.在RL中,我们输入的横坐标一般是step.&lt;/li&gt;
  &lt;li&gt;y 
纵坐标, 我们的纵坐标一般是平均回报.&lt;/li&gt;
  &lt;li&gt;hue&lt;br /&gt;
按照什么标准来分类画图.一般以算法来区分.&lt;/li&gt;
  &lt;li&gt;ci &lt;br /&gt;
置信区间的大小, 设置为”sd”时候,就会画出标准差范围.一般取95.
-error_style&lt;br /&gt;
误差带的类型,可在band和bars之间选择.一般选择band.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Qiang He (何强)</name></author><category term="Reinforcement Learning" /><category term="skills" /><summary type="html">如何绘制出强化学习论文里面的曲线图</summary></entry><entry><title type="html">How to installation mpi4py in your ubuntu</title><link href="https://sweetice.github.io/posts/2019/03/install-mpi4py/" rel="alternate" type="text/html" title="How to installation mpi4py in your ubuntu" /><published>2019-03-18T00:00:00-07:00</published><updated>2019-03-18T00:00:00-07:00</updated><id>https://sweetice.github.io/posts/2019/03/How-to-install-mpi4py-in-your-ubuntu</id><content type="html" xml:base="https://sweetice.github.io/posts/2019/03/install-mpi4py/">&lt;h2 id=&quot;how-to-install-mpi4py-in-your-ubuntu-computer&quot;&gt;How to install mpi4py in your ubuntu computer&lt;/h2&gt;

&lt;p&gt;首先要说明的是，直接使用pip　安装并不可取．&lt;/p&gt;

&lt;h2 id=&quot;1-安装openmpi&quot;&gt;1. 安装openmpi&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1.1 下载
URL: http://www.open-mpi.org/software/ompi/v1.10/&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;wget https://www.open-mpi.org/software/ompi/v1.10/downloads/openmpi-1.10.2.tar.gz
tar xvzf openmpi-1.10.x.tar.gz
cd openmpi-xxx/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;1.2 编译安装&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;首先编译:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./configure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;注意：不要使用&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bash ./configure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;接下来安装:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo make all install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;注意:这里必须要使用sudo,否则会提示你权限不足&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1.3 添加环境变量&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;进入~目录后&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo gedit .bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在最后一行添加上:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#mpi4py
export LD_LIBRARY_PATH+=:/usr/local/lib
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;激活一下:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;1.4 进行测试&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd openmpi-1.10.2/examples
make
mpirun -np 4 hello_c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;当你的电脑上面显示:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Hello, world, I am 0 of 4, (Open MPI v1.10.2, package: Open MPI mirror@agent Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 123)
Hello, world, I am 2 of 4, (Open MPI v1.10.2, package: Open MPI mirror@agent Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 123)
Hello, world, I am 3 of 4, (Open MPI v1.10.2, package: Open MPI mirror@agent Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 123)
Hello, world, I am 1 of 4, (Open MPI v1.10.2, package: Open MPI mirror@agent Distribution, ident: 1.10.2, repo rev: v1.10.1-145-g799148f, Jan 21, 2016, 123)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;这就说明openmpi安装好了.&lt;/p&gt;

&lt;h2 id=&quot;2-安装mpi4py&quot;&gt;2. 安装mpi4py&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;2.1 激活环境变量&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source ~/.bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;2.2 安装mpi4py&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install mpi4py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;出现一下信息:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple
Collecting mpi4py
  Using cached https://pypi.tuna.tsinghua.edu.cn/packages/55/a2/c827b196070e161357b49287fa46d69f25641930fd5f854722319d431843/mpi4py-3.0.1.tar.gz
Building wheels for collected packages: mpi4py
  Building wheel for mpi4py (setup.py) ... done
  Stored in directory: /home/mirror/.cache/pip/wheels/73/ef/7a/e81433083a06d8735f0b50e1e388168b39b88444fd81fe5f27
Successfully built mpi4py
Installing collected packages: mpi4py
Successfully installed mpi4py-3.0.1
You are using pip version 19.0.2, however version 19.0.3 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;说明已经成功安装上了&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2.3 检测是否成功安装&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import mpi4py.MPI as MPI
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;之后再&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dir(MPI)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;出现&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;['AINT', 'ANY_SOURCE', 'ANY_TAG', 'APPNUM', 'Add_error_class', 'Add_error_code', 'Add_error_string', 'Aint_add', 'Aint_diff', 'Alloc_mem', 'Attach_buffer', 'BAND', 'BOOL', 'BOR', 'BOTTOM', 'BSEND_OVERHEAD', 'BXOR', 'BYTE', 'CART', 'CHAR', 'CHARACTER', 'COMBINER_CONTIGUOUS', 'COMBINER_DARRAY', 'COMBINER_DUP', 'COMBINER_F90_COMPLEX', 'COMBINER_F90_INTEGER', 'COMBINER_F90_REAL', 'COMBINER_HINDEXED', 'COMBINER_HINDEXED_BLOCK', 'COMBINER_HVECTOR', 'COMBINER_INDEXED', 'COMBINER_INDEXED_BLOCK', 'COMBINER_NAMED', 'COMBINER_RESIZED', 'COMBINER_STRUCT', 'COMBINER_SUBARRAY', 'COMBINER_VECTOR', 'COMM_NULL', 'COMM_SELF', 'COMM_TYPE_SHARED', 'COMM_WORLD', 'COMPLEX', 'COMPLEX16', 'COMPLEX32', 'COMPLEX4', 'COMPLEX8', 'CONGRUENT', 'COUNT', 'CXX_BOOL', 'CXX_DOUBLE_COMPLEX', 'CXX_FLOAT_COMPLEX', 'CXX_LONG_DOUBLE_COMPLEX', 'C_BOOL', 'C_COMPLEX', 'C_DOUBLE_COMPLEX', 'C_FLOAT_COMPLEX', 'C_LONG_DOUBLE_COMPLEX', 'Cartcomm', 'Close_port', 'Comm', 'Compute_dims', 'DATATYPE_NULL', 'DISPLACEMENT_CURRENT', 'DISP_CUR', 'DISTRIBUTE_BLOCK', 'DISTRIBUTE_CYCLIC', 'DISTRIBUTE_DFLT_DARG', 'DISTRIBUTE_NONE', 'DIST_GRAPH', 'DOUBLE', 'DOUBLE_COMPLEX', 'DOUBLE_INT', 'DOUBLE_PRECISION', 'Datatype', 'Detach_buffer', 'Distgraphcomm', 'ERRHANDLER_NULL', 'ERRORS_ARE_FATAL', 'ERRORS_RETURN', 'ERR_ACCESS', 'ERR_AMODE', 'ERR_ARG', 'ERR_ASSERT', 'ERR_BAD_FILE', 'ERR_BASE', 'ERR_BUFFER', 'ERR_COMM', 'ERR_CONVERSION', 'ERR_COUNT', 'ERR_DIMS', 'ERR_DISP', 'ERR_DUP_DATAREP', 'ERR_FILE', 'ERR_FILE_EXISTS', 'ERR_FILE_IN_USE', 'ERR_GROUP', 'ERR_INFO', 'ERR_INFO_KEY', 'ERR_INFO_NOKEY', 'ERR_INFO_VALUE', 'ERR_INTERN', 'ERR_IN_STATUS', 'ERR_IO', 'ERR_KEYVAL', 'ERR_LASTCODE', 'ERR_LOCKTYPE', 'ERR_NAME', 'ERR_NOT_SAME', 'ERR_NO_MEM', 'ERR_NO_SPACE', 'ERR_NO_SUCH_FILE', 'ERR_OP', 'ERR_OTHER', 'ERR_PENDING', 'ERR_PORT', 'ERR_QUOTA', 'ERR_RANK', 'ERR_READ_ONLY', 'ERR_REQUEST', 'ERR_RMA_ATTACH', 'ERR_RMA_CONFLICT', 'ERR_RMA_FLAVOR', 'ERR_RMA_RANGE', 'ERR_RMA_SHARED', 'ERR_RMA_SYNC', 'ERR_ROOT', 'ERR_SERVICE', 'ERR_SIZE', 'ERR_SPAWN', 'ERR_TAG', 'ERR_TOPOLOGY', 'ERR_TRUNCATE', 'ERR_TYPE', 'ERR_UNKNOWN', 'ERR_UNSUPPORTED_DATAREP', 'ERR_UNSUPPORTED_OPERATION', 'ERR_WIN', 'Errhandler', 'Exception', 'FILE_NULL', 'FLOAT', 'FLOAT_INT', 'F_BOOL', 'F_COMPLEX', 'F_DOUBLE', 'F_DOUBLE_COMPLEX', 'F_FLOAT', 'F_FLOAT_COMPLEX', 'F_INT', 'File', 'Finalize', 'Free_mem', 'GRAPH', 'GROUP_EMPTY', 'GROUP_NULL', 'Get_address', 'Get_error_class', 'Get_error_string', 'Get_library_version', 'Get_processor_name', 'Get_version', 'Graphcomm', 'Grequest', 'Group', 'HOST', 'IDENT', 'INFO_ENV', 'INFO_NULL', 'INT', 'INT16_T', 'INT32_T', 'INT64_T', 'INT8_T', 'INTEGER', 'INTEGER1', 'INTEGER16', 'INTEGER2', 'INTEGER4', 'INTEGER8', 'INT_INT', 'IN_PLACE', 'IO', 'Info', 'Init', 'Init_thread', 'Intercomm', 'Intracomm', 'Is_finalized', 'Is_initialized', 'Is_thread_main', 'KEYVAL_INVALID', 'LAND', 'LASTUSEDCODE', 'LB', 'LOCK_EXCLUSIVE', 'LOCK_SHARED', 'LOGICAL', 'LOGICAL1', 'LOGICAL2', 'LOGICAL4', 'LOGICAL8', 'LONG', 'LONG_DOUBLE', 'LONG_DOUBLE_INT', 'LONG_INT', 'LONG_LONG', 'LOR', 'LXOR', 'Lookup_name', 'MAX', 'MAXLOC', 'MAX_DATAREP_STRING', 'MAX_ERROR_STRING', 'MAX_INFO_KEY', 'MAX_INFO_VAL', 'MAX_LIBRARY_VERSION_STRING', 'MAX_OBJECT_NAME', 'MAX_PORT_NAME', 'MAX_PROCESSOR_NAME', 'MESSAGE_NO_PROC', 'MESSAGE_NULL', 'MIN', 'MINLOC', 'MODE_APPEND', 'MODE_CREATE', 'MODE_DELETE_ON_CLOSE', 'MODE_EXCL', 'MODE_NOCHECK', 'MODE_NOPRECEDE', 'MODE_NOPUT', 'MODE_NOSTORE', 'MODE_NOSUCCEED', 'MODE_RDONLY', 'MODE_RDWR', 'MODE_SEQUENTIAL', 'MODE_UNIQUE_OPEN', 'MODE_WRONLY', 'Message', 'NO_OP', 'OFFSET', 'OP_NULL', 'ORDER_C', 'ORDER_F', 'ORDER_FORTRAN', 'Op', 'Open_port', 'PACKED', 'PROC_NULL', 'PROD', 'Pcontrol', 'Prequest', 'Publish_name', 'Query_thread', 'REAL', 'REAL16', 'REAL2', 'REAL4', 'REAL8', 'REPLACE', 'REQUEST_NULL', 'ROOT', 'Register_datarep', 'Request', 'SEEK_CUR', 'SEEK_END', 'SEEK_SET', 'SHORT', 'SHORT_INT', 'SIGNED_CHAR', 'SIGNED_INT', 'SIGNED_LONG', 'SIGNED_LONG_LONG', 'SIGNED_SHORT', 'SIMILAR', 'SINT16_T', 'SINT32_T', 'SINT64_T', 'SINT8_T', 'SUBVERSION', 'SUCCESS', 'SUM', 'Status', 'TAG_UB', 'THREAD_FUNNELED', 'THREAD_MULTIPLE', 'THREAD_SERIALIZED', 'THREAD_SINGLE', 'TWOINT', 'TYPECLASS_COMPLEX', 'TYPECLASS_INTEGER', 'TYPECLASS_REAL', 'Topocomm', 'UB', 'UINT16_T', 'UINT32_T', 'UINT64_T', 'UINT8_T', 'UNDEFINED', 'UNEQUAL', 'UNIVERSE_SIZE', 'UNSIGNED', 'UNSIGNED_CHAR', 'UNSIGNED_INT', 'UNSIGNED_LONG', 'UNSIGNED_LONG_LONG', 'UNSIGNED_SHORT', 'UNWEIGHTED', 'Unpublish_name', 'VERSION', 'WCHAR', 'WEIGHTS_EMPTY', 'WIN_BASE', 'WIN_CREATE_FLAVOR', 'WIN_DISP_UNIT', 'WIN_FLAVOR', 'WIN_FLAVOR_ALLOCATE', 'WIN_FLAVOR_CREATE', 'WIN_FLAVOR_DYNAMIC', 'WIN_FLAVOR_SHARED', 'WIN_MODEL', 'WIN_NULL', 'WIN_SEPARATE', 'WIN_SIZE', 'WIN_UNIFIED', 'WTIME_IS_GLOBAL', 'Win', 'Wtick', 'Wtime', '__builtins__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__pyx_capi__', '__spec__', '_addressof', '_handleof', '_keyval_registry', '_lock_table', '_set_abort_status', '_sizeof', '_typecode', '_typedict', '_typedict_c', '_typedict_f', 'get_vendor', 'memory', 'pickle']

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这样子就说明安装成功了.&lt;/p&gt;</content><author><name>Qiang He (何强)</name></author><category term="skills" /><summary type="html">How to install mpi4py in your ubuntu computer</summary></entry><entry><title type="html">How to install cuda on ubuntu 1804</title><link href="https://sweetice.github.io/posts/2019/02/how-to-install-cuda-on-ubunutu-1804/" rel="alternate" type="text/html" title="How to install cuda on ubuntu 1804" /><published>2019-02-20T00:00:00-08:00</published><updated>2019-02-20T00:00:00-08:00</updated><id>https://sweetice.github.io/posts/2019/02/How-to-install-cuda-on-ubuntu1804</id><content type="html" xml:base="https://sweetice.github.io/posts/2019/02/how-to-install-cuda-on-ubunutu-1804/">&lt;h2 id=&quot;深度学习环境的安装-ubuntu-1804&quot;&gt;深度学习环境的安装 ubuntu 1804&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;1.查看是否支持CUDA，清理掉原来的驱动&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用如下命令：&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;lspci | grep -i nvidia
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;我得到的输出是：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;01:00.0 VGA compatible controller: NVIDIA Corporation GP104 [GeForce GTX 1070 Ti] (rev a1)
01:00.1 Audio device: NVIDIA Corporation GP104 High Definition Audio Controller (rev a1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;说明我的显卡是1070Ti。&lt;/p&gt;

&lt;p&gt;再清理掉原来的驱动&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get autoremove --purge nvidia*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;2.禁用nouveau
在终端运行&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ lsmod | grep nouveau
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果有输出，说明nouveau正在运行，需要手动禁掉nouveau。&lt;/p&gt;

&lt;p&gt;使用&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo vim /etc/modprobe.d/blacklist-nouveau.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在文件内写入：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;blacklist nouveau 
options nouveau modeset=0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;执行&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo update-initramfs -u
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;确定是否禁用成功&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ lsmod | grep nouveau
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;3.安装驱动&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;此处建议使用ubuntu自带的software &amp;amp; updates安装驱动。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1 打开Software &amp;amp; Updates&lt;/li&gt;
  &lt;li&gt;2 点击Additional Drivers&lt;/li&gt;
  &lt;li&gt;3 选用 Using NVIDIA driver metapackage from nvidia-diver-390(proprietary, tested)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;或者考虑一下方式:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa      //添加ppa库到系统中
sudo apt update         //  更新
 
sudo ubuntu-drivers devices // 显示可以安装的nvidia驱动
 
 
sudo apt install nvidia-xxx        //  xxx  代表你想安装的nVidia驱动的版本号
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/sweetice/sweetice.github.io/blob/master/figures/software%20and%20updates.png&quot; alt=&quot;Software and Updates&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如果没有成功，那么就去&lt;a href=&quot;https://www.nvidia.cn/Download/index.aspx?lang=cn&quot;&gt;官网&lt;/a&gt;选择对应的版本安装。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://github.com/sweetice/sweetice.github.io/blob/master/figures/nvidia-driver.png&quot; alt=&quot;nvidia-driver&quot; /&gt;&lt;/p&gt;

&lt;p&gt;下载好之后，使用alt+ctrl+f1切换到tty1模式去，从ubuntu1804开始，可以不禁用lightdm了。&lt;/p&gt;

&lt;p&gt;使用如下的命令安装&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo bash ./NVIDIA-Linux-x86_64-410.93.run 

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;安装完成之后，切换回图形界面。
接下来。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;4.安装cuda&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;上&lt;a href=&quot;https://developer.nvidia.com/cuda-90-download-archive&quot;&gt;官网&lt;/a&gt;下载cuda9.0的包。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://developer.nvidia.com/cuda-90-download-archive
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;之后，关闭掉图形界面（方法同上），进入下载目录，执行如下命令：&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo ./cuda_9.0_linux.run --verbose -silent --toolkit --override 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;一定要加入后面的参数&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这样子不用经过繁琐的选择，能够直接安装上cuda。&lt;/p&gt;

&lt;p&gt;现在你还缺少了一些库，安装他们&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;快要成功了，接下来添加一下环境变量&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd ~
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo gedit ./bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;在文件的最后，加上：&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export PATH=&quot;$PATH:/usr/local/cuda-9.0/bin&quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;然后退出，至此cuda安装完成。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;5.安装cudNN&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;去&lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-download&quot;&gt;官网&lt;/a&gt;下载cudnn（注意这里我们下载.tgz版本）&lt;/li&gt;
  &lt;li&gt;下载好之后解压到本地&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$sudo tar -xzvf cudnn-9.0-linux-x64-v7.5.0.56.tgz 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;将文件拷贝至cuda的目录中
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo cp cuda/include/cudnn.h /usr/local/cuda/include
sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;配置相应的环境变量&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gedit .bashrc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;把这个加在最后一行：&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;6.测试&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;使用如下代码测试&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import torch
torch.cuda.is_available()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;如果结果显示True，那么就可以使用了。&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow as tf
a = tf.constant([1.0,2.0,3.0],shape=[3],name='a')
b = tf.constant([1.0,2.0,3.0],shape=[3],name='b')
with tf.device('/gpu:1'):
    c = a + b
sess = tf.Session()
sess.run(tf.global_variables_initializer())
print(sess.run(c))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/sweetice/sweetice.github.io/blob/master/figures/cuda_is_available.png&quot; alt=&quot;cuda可以用了&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;一些你可能用到的命令&quot;&gt;一些你可能用到的命令&lt;/h2&gt;

&lt;p&gt;查看占用apt进程的命令&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ps aux | grep -i apt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;杀死这种命令&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kill -9 id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Qiang He (何强)</name></author><category term="skills" /><summary type="html">深度学习环境的安装 ubuntu 1804</summary></entry><entry><title type="html">Computing the gradients of gradients with pytorch</title><link href="https://sweetice.github.io/posts/2019/01/compute-gradients-of-gradients/" rel="alternate" type="text/html" title="Computing the gradients of gradients with pytorch" /><published>2019-01-26T00:00:00-08:00</published><updated>2019-01-26T00:00:00-08:00</updated><id>https://sweetice.github.io/posts/2019/01/Compute%20the%20gradients%20of%20gradients%20with%20pytorch</id><content type="html" xml:base="https://sweetice.github.io/posts/2019/01/compute-gradients-of-gradients/">&lt;h2 id=&quot;computing-the-gradients-of-gradients-with-pytorch&quot;&gt;Computing the gradients of gradients with pytorch&lt;/h2&gt;

&lt;p&gt;Rencently, I am working with GAN and RL.For the puspose of smoothing the learning process, I need compute the gradients of gradients(the second order of gradients).&lt;/p&gt;

&lt;p&gt;If you are fimilar with WGAN, you must know the lipschitz constrain. Implementation with GAN-gp, you need compute the gradients of D as constrain.&lt;/p&gt;

&lt;p&gt;That is easy for you if you work with Tensorflow (I hate this tool!).&lt;/p&gt;

&lt;p&gt;Pytorch is a useful tool, I love it.&lt;/p&gt;

&lt;p&gt;We can use torch.autograd.grad to implement the code.&lt;/p&gt;

&lt;p&gt;You can click &lt;a href=&quot;https://pytorch.org/docs/0.4.1/autograd.html#&quot;&gt;here&lt;/a&gt; to read the docs.&lt;/p&gt;

&lt;p&gt;Let’s read the core code.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)[source]
Computes and returns the sum of gradients of outputs w.r.t. the inputs.

grad_outputs should be a sequence of length matching output containing the pre-computed gradients w.r.t. each of the outputs. If an output doesn’t require_grad, then the gradient can be None).

If only_inputs is True, the function will only return a list of gradients w.r.t the specified inputs. If it’s False, then gradient w.r.t. all remaining leaves will still be computed, and will be accumulated into their .grad attribute.

Parameters:	
outputs (sequence of Tensor) – outputs of the differentiated function.
inputs (sequence of Tensor) – Inputs w.r.t. which the gradient will be returned (and not accumulated into .grad).
grad_outputs (sequence of Tensor) – Gradients w.r.t. each output. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional. Default: None.
retain_graph (bool, optional) – If False, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph.
create_graph (bool, optional) – If True, graph of the derivative will be constructed, allowing to compute higher order derivative products. Default: False.
allow_unused (bool, optional) – If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s ok. Here I give you guys two example.&lt;/p&gt;

&lt;h2 id=&quot;example-1-gan-gp-&quot;&gt;Example 1: GAN-gp :&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def compute_gradient_penalty(D, real_samples, fake_samples):
    &quot;&quot;&quot;Calculates the gradient penalty loss for WGAN GP&quot;&quot;&quot;
    # Random weight term for interpolation between real and fake samples
    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))
    # Get random interpolation between real and fake samples
    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)
    d_interpolates = D(interpolates)
    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)
    # Get gradient w.r.t. interpolates
    gradients = autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=fake,
        create_graph=True,
        retain_graph=True,
        only_inputs=True,
    )[0]
    gradients = gradients.view(gradients.size(0), -1)
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
    return gradient_penalty
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you are interested in WGAN-gp, you can click &lt;a href=&quot;https://github.com/sweetice/GAN-course-note/blob/master/GAN-code/wgan-gp.py&quot;&gt;here&lt;/a&gt; to read the core code.&lt;/p&gt;

&lt;h2 id=&quot;example-2-in-any-network-&quot;&gt;Example 2: In any network :&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def compute_gradient_penalty(self, network, input):
        # ref: https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py
        '''

        :param input: state[index]
        :param network: actor or critic
        :return: gradient penalty
        '''
        input_ = torch.tensor(input).requires_grad_(True)
        output = network(input_)
        musk = torch.ones_like(output)
        gradients = grad(output, input_, grad_outputs=musk,
                         retain_graph=True, create_graph=True,
                         allow_unused=True)[0]  # get tensor from tuple
        gradients = gradients.view(-1, 1)
        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()
        return gradient_penalty
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you are interested in reinforcement learning and general aritificial intelligence, you can click &lt;a href=&quot;https://github.com/sweetice/Deep-reinforcement-learning-with-pytorch&quot;&gt;here&lt;/a&gt;. :)&lt;/p&gt;

&lt;p&gt;Enjoy yourself :)&lt;/p&gt;</content><author><name>Qiang He (何强)</name></author><category term="Reinforcement Learning" /><category term="Pytorch" /><summary type="html">Computing the gradients of gradients with pytorch</summary></entry><entry><title type="html"></title><link href="https://sweetice.github.io/2019-03-13-NVIDIA-SMI/" rel="alternate" type="text/html" title="" /><published></published><updated></updated><id>https://sweetice.github.io/2019-03-13-NVIDIA-SMI</id><content type="html" xml:base="https://sweetice.github.io/2019-03-13-NVIDIA-SMI/">&lt;p&gt;#　NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running&lt;/p&gt;

&lt;p&gt;#＃ 问题描述：&lt;/p&gt;

&lt;p&gt;某次使用电脑之后, 打开电脑之后输入nvidia-smi 提示&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;##　问题原因
这常常是因为用户在程序运行时强行关闭电脑，或者电脑断电而引起的．&lt;/p&gt;

&lt;h2 id=&quot;解决方案&quot;&gt;解决方案&lt;/h2&gt;

&lt;p&gt;大致就是重装一下驱动就可以&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;１. 卸载原有驱动
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get autoremove --purge nvidia*
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;增加ppa源&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa
sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;检测显卡版本及推荐的驱动&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ubuntu-drivers devices
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;结果显示&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;== /sys/devices/pci0000:00/0000:00:01.0/0000:01:00.0 ==
modalias : pci:v000010DEd00001B82sv00001458sd00003794bc03sc00i00
vendor   : NVIDIA Corporation
model    : GP104 [GeForce GTX 1070 Ti]
driver   : nvidia-driver-390 - distro non-free recommended
driver   : xserver-xorg-video-nouveau - distro free builtin

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;说明我这里应该去安装390的驱动&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;重装驱动&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt-get install nvidia-driver-390
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;ol&gt;
      &lt;li&gt;重启
这是最重要的一步,如果你不重启,那么重装之后立即nvidia-smi是会发现现在还是没有办法使用的,重启之后就好了&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;nvidia-smi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.116                Driver Version: 390.116                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 107...  Off  | 00000000:01:00.0 Off |                  N/A |
|  0%   55C    P2    40W / 180W |   1046MiB /  8119MiB |     19%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1401      G   /usr/lib/xorg/Xorg                           249MiB |
|    0      1573      G   /usr/bin/gnome-shell                         195MiB |
|    0      2509      C   /home/mirror/anaconda3/bin/python            533MiB |
|    0      2562      G   ...quest-channel-token=9028700102029312797    64MiB |
+-----------------------------------------------------------------------------+
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Qiang He (何强)</name></author></entry></feed>