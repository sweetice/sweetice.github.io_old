pub_date	title	venue	excerpt	citation	url_slug	paper_url
2020-08-25	Reducing Estimation Bias via Weighted Delayed Deep Deterministic Policy Gradient	ICTAI’20, Oral	We prove that underestimation bias occurs when taking the minimum of a pair of action-value functions.	He Q, Hou X. Reducing Estimation Bias via Weighted Delayed Deep Deterministic Policy Gradient[J]. arXiv preprint arXiv:2006.12622, 2020.	paper-title-number-1	paper1.pdf
2020-09-25	POPO: Pessimistic Offline Policy Optimization	NeurIPS’20, WS	We show that a challenge arises when applying value-based algorithms to completely offline data that the explosion of the value function. When we evaluate the value function, the error caused by actions out of the distribution will not be eliminated because the inability to interact with the environment makes it impossible to eliminate this error through the Bellman equation.	Coming Soon	paper-title-number-2	paper2.pdf
2020-10-25	Optimization of Bellman Error Distribution based on SVGD in RL	Underreview	We propose Stein variational gradient descend method that optimizes the Bellman error distribution in the RL algorithm to stabilize the	Coming Soon	paper-title-number-4	paper4.pdf
2020-12-12	Wide-sense Stationary Policy Optimization with Bellman Residual on Video Games	ICME’21, Oral	We propose Quantile Regression method to optimize to Bellman Residual Distribution to stabilize the training process	Coming Soon	paper-title-number-4	paper5.pdf
