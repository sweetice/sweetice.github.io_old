

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Functional Gradient Descent - Qiang He’s Homepage (CASIA)</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Qiang He's Homepage (CASIA)">
<meta property="og:title" content="Functional Gradient Descent">


  <link rel="canonical" href="https://sweetice.github.io/posts/2021/03/functional-gradient-descent/">
  <meta property="og:url" content="https://sweetice.github.io/posts/2021/03/functional-gradient-descent/">



  <meta property="og:description" content="Claim: This is a backup of  this post!">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2021-03-12T00:00:00-08:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Qiang He (何强)",
      "url" : "https://sweetice.github.io",
      "sameAs" : null
    }
  </script>



  <meta name="google-site-verification" content="google-site-verification=6CU-DoIp0FraaNmQsqgvxCw5swk1BHoouEqmFRoeWNc" />




<!-- end SEO -->


<link href="https://sweetice.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Qiang He's Homepage (CASIA) Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://sweetice.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->
<meta name="google-site-verification" content="r78CFkv8ENH5zPM3DqSoC5yKA2XG3eGnIypcz7l-dIg" />
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-N65T6R1ZKW"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N65T6R1ZKW');
</script>


<link rel="apple-touch-icon" sizes="57x57" href="https://sweetice.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://sweetice.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://sweetice.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://sweetice.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://sweetice.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://sweetice.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://sweetice.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://sweetice.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://sweetice.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://sweetice.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://sweetice.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://sweetice.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://sweetice.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://sweetice.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://sweetice.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://sweetice.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://sweetice.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://sweetice.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

    <meta name="google-site-verification" content="r78CFkv8ENH5zPM3DqSoC5yKA2XG3eGnIypcz7l-dIg" />
  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://sweetice.github.io/">Qiang He's Homepage (CASIA)</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://sweetice.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://sweetice.github.io/talks/">Talks</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://sweetice.github.io/year-archive/">Blog Posts</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://sweetice.github.io/cv/">CV</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://sweetice.github.io/images/qianghe2017.jpg" class="author__avatar" alt="Qiang He (何强)">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Qiang He (何强)</h3>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Beijing, China</li>
      
      
      
      
      
       
      
      
      
      
      
      
      
      
      
        <li><a href="https://github.com/sweetice"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.com.hk/citations?user=l6Y2ZDYAAAAJ&hl=zh-CN"><i class="fas fa-fw fa-graduation-cap"></i> Google Scholar</a></li>
      
      
      
      
      
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Functional Gradient Descent">
    <meta itemprop="description" content="Claim: This is a backup of  this post!">
    <meta itemprop="datePublished" content="March 12, 2021">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Functional Gradient Descent
</h1>
          
            <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  14 minute read
	
</p>
          
        
        
        
          <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2021-03-12T00:00:00-08:00">March 12, 2021</time></p>
        
        
             
        
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>Claim: This is a backup of  <a href="https://simple-complexities.github.io/optimization/functional/gradient/descent/2020/03/04/functional-gradient-descent.html">this post</a>!</p>

<p>The content in this post has been adapted from <a href="http://www.cs.cmu.edu/~16831-f12/notes/F12/16831_lecture21_danielsm.pdf">Functional Gradient Descent - Part 1</a> and <a href="http://www.cs.cmu.edu/~16831-f14/notes/F10/16831_lecture24_varunnr/16831_lectureNov11.vramakri.pdf">Part 2</a>.
Functional Gradient Descent was introduced in the NIPS publication <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf">Boosting Algorithms as Gradient Descent</a> by Llew Mason, Jonathan Baxter, Peter Bartlett and Marcus Frean in the year 2000.</p>

<p>We are all familiar with gradient descent for linear functions \(f(x) = w^Tx\).<br />
Once we define a loss \(L\), gradient descent does the following update steps (\(\eta\) is a parameter called the learning rate.):
\[
    w \rightarrow w - \eta \nabla L(w)
\]
where we move around in the space of weights.
An example of a loss \(L\) is:
\[
    L(w) = \sum_{i=1}^n(y_i - w^Tx_i)^2 + \lambda\lVert w \rVert ^2
\]
where the first term (the ‘L2’ term) measures how close \(f(x)\) is to \(y\), while the second term (the ‘regularization’ term) accounts for the ‘complexity’ of the learned function \(f\).</p>

<p>Suppose we wanted to extend \(L\) to beyond linear functions \(f\). We want to minimize something like:
\[
    L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2
\]
where \(\lVert f \rVert ^2\) again serves as a regularization term, and we have updates of the form:
\[
    f \rightarrow f - \eta \nabla L(f)
\]
where we move around in the space of functions, not weights!</p>

<p>Turns out, this is completely possible! And goes by the name of ‘functional’ gradient descent, or gradient descent in function space.</p>

<p>A question that one may have is: why do this in the first place? Every function can be parametrized, and we can do ‘ordinary’ gradient descent in the space of parameters, instead?</p>

<p>The answer is: yes, you always can! In general, you can parametrize any function in a number of ways, each parametrization gives rise to different steps (and different functions at each step) in gradient descent.</p>

<p>The advantage is that some loss functions that are non-convex when parametrized, can be convex in the function space: this means functional gradient descent can actually converge to global minima, when ‘ordinary’ gradient descent could possibly get stuck at local minima or saddle points.</p>

<p>So, what does functional gradient descent mean?</p>

<h3 id="functionals">Functionals</h3>
<p>A functional is a function defined over functions, returning a real value.<br />
Examples:</p>
<ul>
  <li>The evaluation functional
\[
  E_x(f) = f(x)
\]</li>
  <li>The sum functional
\[
  S_{{x_1, \ldots, x_n}}(f) = \sum_{i = 1}^n f(x_i) dx
\]</li>
  <li>The integration functional
\[
  I_{[a, b]}(f) = \int_a^b f(x) dx
\]</li>
</ul>

<p>It follows that the composition of a function \(g: \mathbb{R} \to \mathbb{R}\) with a functional is also a functional.
The loss function \(L(f)\) defined above is a functional!</p>

<h3 id="reproducing-kernel-hilbert-space">Reproducing Kernel Hilbert Space</h3>
<p>It turns out that it is especially convenient when functions come from a special set, called a reproducing kernel Hilbert space.
A Hilbert space can be thought of as a vector space, where we have the notion of an inner product between two elements. (This is not the complete definition, but this is what we’ll need).</p>

<hr />

<h4 id="review-kernels">Review: Kernels</h4>
<p>A kernel \(K: X \times X \to \mathbb{R}\) is a function that generalizes dot products:</p>
<ul>
  <li><em>Symmetry</em>: For any \(x_i, x_j \in X\):
\[
  K(x_i, x_j) = K(x_j, x_i).
\]</li>
  <li><em>Positive Semi-Definiteness</em>: For any \(x_1, \ldots, x_n \in X\), the matrix \(K_M\) given by 
\[
  K_{M_{ij}} = K(x_i, x_j)
\]
is positive semi-definite. Note that this implies \(K(x_i, x_j) \geq 0\) always.</li>
</ul>

<p>It turns out (Mercer’s condition) that these conditions are equivalent to
\[
    K(x_i, x_j) = \phi(x_i) \cdot \phi(x_j)
\]
where \(\phi\) is a function that is sometimes called the ‘feature map’. Thus, a kernel can be thought of as the dot product in some feature space (the range of \(\phi\)).
Similar to the dot product then, the kernel measures similarity between two inputs.</p>

<p>Examples of kernel functions include:</p>
<ul>
  <li>Linear Kernel: 
\[
  K(x_i, x_j) = x_i \cdot x_j 
\]</li>
  <li>Polynomial Kernel (of degree \(d\)): 
\[
  K(x_i, x_j) = (x_i \cdot x_j + c)^d 
\]
The presence of the \(c\) term allows coefficients of degree less than \(d\) to be accommodated too.</li>
  <li>RBF Kernel (of ‘width’ \(\sigma\)): 
\[
  K(x_i, x_j) = \exp\left(-\frac{\lVert x_i - x_j \rVert^2}{2\sigma^2}\right)
\]</li>
</ul>

<p>Try to derive what the associated feature map is, for each of these kernels!</p>

<hr />
<p> </p>

<p>We can now define a reproducing kernel Hilbert space or a ‘RKHS’.<br />
A <em>reproducing kernel Hilbert space</em>, obtained on fixing a kernel \(K\), is a space of functions where every function \(f\) is some linear combination of the kernel \(K\) evaluated at some ‘centers’ \(x_{Cf}\):
\[
    f(x) = \sum_{i = 1}^n \alpha_{f_i} K(x, x_{Cf_i}) 
\]
or, ignoring the argument \(x\):
\[
    f = \sum_{i = 1}^n \alpha_{f_i} K(\cdot, x_{Cf_i}) 
\]
For a kernel \(K\) will denote the associated reproducing kernel Hilbert space by \(H_K\).<br />
From the definition above, every \(f \in H_K\) is completely determined by the coefficients \(\alpha_f\) and the centers \(x_{Cf}\). Note that the number of centers (\(=\) dimension of \(\alpha_f\)) can vary between functions.</p>

<p>We can now define the inner product (the ‘dot’ product) in \(H_K\) by:
\[
    f \cdot g =  \sum_{i = 1}^{n_f} \sum_{j = 1}^{n_g} \alpha_{f_i} \alpha_{g_j} K(x_{Cf_i}, x_{Cg_j}) = \alpha_f K_{fg} \alpha_g
\]
where,
\[
    K_{{fg}_{ij}} = K(x_{Cf_i}, x_{Cg_j}).
\]
This inner product induces the norm \(\lVert \cdot \rVert\):
\[
    {\lVert f \rVert}^2 = f \cdot f = \alpha_f K_{ff} \alpha_f \geq 0.
\]
Why do we use the term reproducing? This is because we can ‘reproduce’ the value of \(f \in H_K\) at any \(x\) by taking the inner product of \(f\) with the ‘reproducing kernel’ function \(K(x, \cdot) \in H_K\):
\[
    f \cdot K(x, \cdot) = f(x).
\]
Verify this property!</p>

<p>So, we’ve seen how to define the inner-product and norm \(\lVert f \rVert\) of any function \(f \in H_K\). But, in order to minimize via gradient descent, we need the definition of a derivative.</p>

<h3 id="derivatives-of-functionals">Derivatives of Functionals</h3>
<p>As reviewed in my previous post on <a href="/optimization/constrained/theory/2020/03/03/optimization-review.html">optimization theory</a>, one of the definitions of the derivative \(Df\) of a function \(f: \mathbb{R}^n \rightarrow \mathbb{R}\) is:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{f(x + h) - (f(x) + Df(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0
\]
where \(Df(x)\) is a size \(n\) row vector, with which the take the dot product of the direction \(h\) with.</p>

<p>We may not be working in \(\mathbb{R}^n\) anymore, but this definition gives us that the derivative \(DE\) of a functional \(E\) on \(H_K\) must satisfy:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(x + h) - (E(x) + DE(x) \cdot h)}\rVert}{\lVert{h}\rVert} = 0
\]
where \(h\) and \(x\) are now functions in \(H_K\), instead of points. This means that \(DE(x)\) is a function, too! (Recall that addition of functions occurs point-wise.)</p>

<hr />
<p><strong>Example 1:</strong>
Let us take the example of the evaluation functional \(E_x(f) = f(x)\) and compute its derivative:
\[
    \begin{aligned}
        E(f + h) &amp;= (f + h)(x)  \newline
                 &amp;= f(x) + h(x) \newline
                 &amp;= E(f) + h(x) \newline
                 &amp;= E(f) + K(x, \cdot) \cdot h \newline
    \end{aligned}
\]
Thus, the derivative \(DE_x(f)\) is independent of \(f\), and is given by:
\[
        DE_x(f) = K(x, \cdot).
\]</p>

<p><strong>Example 2:</strong>
Similarly, following the example of my reference material, the functional \(E(f) = {\lVert f \rVert}^2\) satisfies:
\[
    \begin{aligned}
        E(f + h) &amp;= {\lVert f + h \rVert}^2  \newline
                 &amp;= {(f + h) \cdot (f + h)} \newline
                 &amp;= {f \cdot f} + 2 {f \cdot h} + {h \cdot h} \newline
                 &amp;= E(f) + 2 {f \cdot h} + {h \cdot h} \newline
    \end{aligned}
\]
Thus, the derivative \(DE(f)\) is defined as:
\[
        DE(f) = 2f.
\]
Note how similar this is to the derivative \(2x\) of the function \(x \to \lvert x \rvert^2\) on real numbers!</p>

<hr />
<p> </p>

<h4 id="the-chain-rule">The Chain Rule</h4>
<p>Very fortunately, we also have the chain rule!
As discussed before, if \(E\) is a functional and \(g: \mathbb{R} \to \mathbb{R}\) is differentiable, then \(g(E)\) is also a functional, with derivative:
\[
    D(g(E))(f) = g’(E(f)) \ DE(f).
\]</p>

<hr />

<p><strong>Example 3:</strong>
Let us compute the derivative of the loss functional \(L(f) = \sum_{i=1}^n(y_i - f(x_i))^2 + \lambda\lVert f \rVert ^2\) with the chain rule:
The individual terms in the first sum term is a composition of
\[
    g_i(x) = (y_i - x)^2 \text{ and } E_{x_i}.
\]
Thus, each of these terms has derivative:
\[
    \begin{aligned}
    D(g_i({x_i}))(f) &amp;= -2 (y_i - E_{x_i}(f)) \cdot DE_{x_i}(f) \newline
                     &amp;= -2 (y_i - f(x_i)) \cdot K(x_i, \cdot).
    \end{aligned}
\]
The second term has derivative \(2\lambda f\), as derived above.
Thus, the derivative \(DL(f)\) is given by:
\[
    DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f.
\]</p>

<hr />
<p> </p>

<p>There is one last point to note. When we take steps in ‘ordinary’ gradient descent, we move along the negative of the gradient vector because that is the direction along which the dot product with the gradient is minimum. No matter how this vector points! In some sense, we are not restricted to move in any direction. This is because our underlying domain is \(\mathbb{R}^n\).</p>

<p>In ‘functional’ gradient descent, however, we are restricted to \(H_K\). How can we guarantee that when moving along \(DL(f)\), we do not stray out of \(H_K\)? One way to ensure that is by proving that we always have \(DL(f) \in H_K\). (This was true for our examples above! We have actually implicitly assumed this in our definition, too.) Then, closure of \(H_K\) under addition (and scalar multiplication) ensures that at every iteration, our current function \(f\) is in \(H_K\).</p>

<p>This is what we will prove, in the next section.</p>

<p>The <a href="https://papers.nips.cc/paper/1766-boosting-algorithms-as-gradient-descent.pdf">Boosting Algorithms as Gradient Descent</a> paper above, does not use reproducing kernel Hilbert spaces, and actually applies to more general sets of functions. This is why they mention the fact that moving along the gradient is not always possible. Instead, they move along the direction with the least dot product with the gradient, among all directions that keeps them within their domain of functions. With reproducing kernel Hilbert spaces, this is not a problem: we are, fortunately, not restricted to move along the negative of the gradient.</p>

<h3 id="h_k-is-closed-under-the-derivative">\(H_K\) is Closed under the Derivative</h3>
<p>If \(E\) is a functional on \(H_K\), and \(f \in H_K\), then we always have:
\[
    DE(f) \in H_K.
\]</p>

<p>Let us define the derivative \(DE^*(f)\) as a functional:
\[
    \lim_{\lVert h \rVert \rightarrow 0} \frac{\lVert{E(f + h) - (E(f) + DE^*(f)(h))}\rVert}{\lVert{h}\rVert} = 0
\]
I differentiate between \(DE^*(f)\) (the functional) and \(DE(f)\) (the function).
We want to show that, in fact:
\[
    DE^*(f)(h) = {\langle h, DE(f) \rangle}_K
\]</p>

<p>Note that \(DE^*(f)\) is a linear functional!
Why? Using the definition above (and properties of the norm and limits):</p>
<ul>
  <li>\(DE^*(f)(ch) = c \cdot DE^*(f)(h)\) where \(c \in \mathbb{R}\).</li>
  <li>\(DE^*(f)(h + g) = DE^*(f)(h) + DE^*(f)(g)\).</li>
</ul>

<p>This should not be surprising! We use \(DE^*(f)\) to give us the ‘best’ linear approximation around \(f\) along each direction \(h\).</p>

<p>The <a href="https://en.wikipedia.org/wiki/Riesz_representation_theorem">Riesz Representation Theorem</a> then tells us that every linear functional \(L\) on a Hilbert space is actually of the form:
\[
    L = \langle \cdot, v \rangle
\]
for some \(v\) in the Hilbert space, where \(\langle \cdot, \cdot \rangle\) is the inner product in the Hilbert space.<br />
For an RKHS, the inner product is given by the kernel \(K\), so,
\[
    DE^*(f) = {\langle \cdot, DE(f) \rangle}_K
\]
for some \(DE(f) \in H_K\).
This means, we can write:
\[
    DE^*(f)(h) = {\langle h, DE(f) \rangle}_K
\]
where \(DE(f) \in H_K\), which is what we had to show!</p>

<h3 id="an-example">An Example</h3>
<p>Consider the regression problem, where \(x_i,\) for \(i \in {1, \ldots, 20}\) are linearly spaced in \([-1, 1]\):
\[
    y_i = e^{-\left(\frac{x_i - 0.5}{0.5}\right)^2} + e^{-\left(\frac{x_i + 0.5}{0.5}\right)^2} + \frac{\mathcal{N}(0, 1)}{20}
\]</p>

<p>Although this is a simple enough problem that would be easily solved by ‘ordinary’ gradient descent, we will demonstrate how functional gradient descent works here.</p>

<p>First, we need a loss function. We will use the L2 loss with regularization, \(L(f)\), defined above. We have already seen in Example 3, that the gradient of \(L(f)\) is:
\[
    DL(f) = \sum_{i = 1}^n -2 (y_i - f(x_i)) \cdot K(x_i, \cdot) + 2\lambda f.
\]
Let us define \(K\) as the RBF kernel with width \(0.5\). The presence of the Gaussian noise term above means the true hypothesis is not in \(H_K\), but we should get close!</p>

<p>We initialize \(\alpha_{f_0}\) randomly, and set:
\[ 
    f_0 = \sum_{i = 1}^{20} \alpha_{f_0i} K(\cdot, x_i) 
\]
and then start updating:
\[
    f_{t + 1} = f_t - \eta \cdot DL(f_t)
\]</p>

<p>This makes sense! But if we want to represent this in code, we would have to represent these functions in some way. One way is to maintain the coefficients \(\alpha_{f_t}\) and kernel centers \(x_{C{f_t}}\) at every iteration. We can simplify this by deciding to store only \(\alpha_{f_t}\) (allowing zeros) and implicitly use all \(x_i\) as the kernel centers. This is actually the same as doing gradient descent on \(\alpha_f\)! Other ways would be to add training samples one-by-one in an online manner, maintaining/recomputing the function values only at the training points. However, this causes a complication wherein the function is only defined at the training samples. To fix this, instead of updating by the gradient, we update using smooth functions that approximate the gradient: this is exactly gradient boosting!</p>

<p>If we decide to represent our function implicitly by \(\alpha_f\) at each step, our updates are now:
\[
    \alpha_{f_{t + 1}} = 2 \eta (y - f_t(x)) +  (1 - 2\lambda\eta) \alpha_{f_t}
\]
where \(y - f_t(x)\) is a vector with \((y - f_t(x))_i = y_i - f_t(x_i)\).
Check this!</p>

<p>If we implement all this, and plot the resulting learned hypothesis at each step of gradient descent:</p>

<p style="text-align:center"><img src="/assets/images/functional_gradient_descent.gif" alt="Functional Gradient Descent Example" title="Functional Gradient Descent Example" /></p>

<p>indicating that functional gradient descent converges pretty fast, for our example.
The code for this example is available <a href="https://github.com/simple-complexities/simple-complexities.github.io/tree/master/code/functional_gradient_descent.py">here</a>.</p>

<h3 id="conclusion">Conclusion</h3>
<p>We have seen:</p>
<ul>
  <li>Why functional gradient descent can be useful,</li>
  <li>What it means to do functional gradient descent, and,</li>
  <li>How we can do functional gradient descent, with an example.</li>
</ul>

<p>and that’s all I have for today.</p>

<p>I’m also starting to introduce a commenting facility via GitHub Issues, in order to not clutter up this space here. Comment <a href="https://github.com/simple-complexities/simple-complexities.github.io/issues/3">here</a>!</p>

        
      </section>

      <footer class="page__meta">
        
        


  




  
  
  

  <p class="page__taxonomy">
    <strong><i class="fa fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="https://sweetice.github.io/tags/#optimization" class="page__taxonomy-item" rel="tag">Optimization</a>
    
    </span>
  </p>




      </footer>

      

<section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=https://sweetice.github.io/posts/2021/03/functional-gradient-descent/" class="btn btn--twitter" title="Share on Twitter"><i class="fab fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https://sweetice.github.io/posts/2021/03/functional-gradient-descent/" class="btn btn--facebook" title="Share on Facebook"><i class="fab fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://sweetice.github.io/posts/2021/03/functional-gradient-descent/" class="btn btn--linkedin" title="Share on LinkedIn"><i class="fab fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>

      


  <nav class="pagination">
    
      <a href="https://sweetice.github.io/posts/2019/12/skills-on-ubuntu/" class="pagination--pager" title="配置ubuntu系统常用技能
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      
        <h4 class="page__related-title">You May Also Enjoy</h4>
      
      <div class="grid__wrapper">
        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://sweetice.github.io/posts/2019/12/skills-on-ubuntu/" rel="permalink">配置ubuntu系统常用技能
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-12-25T00:00:00-08:00">December 25, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h2 id="配置静态ipv6地址">配置静态ipv6地址</h2>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://sweetice.github.io/posts/2019/04/plot-performance-figures-in-rl/" rel="permalink">How to plot performance figures in reinforcement learning papers
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  less than 1 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-04-19T00:00:00-07:00">April 19, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h1 id="如何绘制出强化学习论文里面的曲线图">如何绘制出强化学习论文里面的曲线图</h1>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://sweetice.github.io/posts/2019/03/install-mpi4py/" rel="permalink">How to installation mpi4py in your ubuntu
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  3 minute read
	
</p>
    

        
         <p class="page__date"><strong><i class="fa fa-fw fa-calendar" aria-hidden="true"></i> Published:</strong> <time datetime="2019-03-18T00:00:00-07:00">March 18, 2019</time></p>
        

    
    <p class="archive__item-excerpt" itemprop="description"><h2 id="how-to-install-mpi4py-in-your-ubuntu-computer">How to install mpi4py in your ubuntu computer</h2>

</p>
    
    
    

  </article>
</div>

        
          





<div class="grid__item">
  <article class="archive__item" itemscope itemtype="http://schema.org/CreativeWork">
    

    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="https://sweetice.github.io/2019-03-13-NVIDIA-SMI/" rel="permalink">
</a>
      
    </h2>
    
    
      <p class="page__meta"><i class="fa fa-clock-o" aria-hidden="true"></i> 


  
	  1 minute read
	
</p>
    

        

    
    <p class="archive__item-excerpt" itemprop="description">
</p>
    
    
    

  </article>
</div>

        
      </div>
    </div>
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<a href="/sitemap/">Sitemap</a>
<!-- end custom footer snippets -->

        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    
    
    
    
      <li><a href="http://github.com/sweetice"><i class="fab fa-github" aria-hidden="true"></i> GitHub</a></li>
    
    
    <li><a href="https://sweetice.github.io/feed.xml"><i class="fa fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 Qiang He (何强). Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://sweetice.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'G-VRTHF8HTJD', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

